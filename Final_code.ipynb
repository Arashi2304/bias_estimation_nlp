{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set model\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentence completion to identify bias when task is not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_length\": 100,\n",
    "}\n",
    "\n",
    "task1_folder = \"task1\"\n",
    "\n",
    "results_folder = os.path.join(task1_folder, model_name)\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(task1_folder):\n",
    "    if filename.endswith(\"_task1_prompts.txt\"):\n",
    "        file_path = os.path.join(task1_folder, filename)\n",
    "        \n",
    "        with open(file_path, \"r\") as file:\n",
    "            prompts = [line.strip() for line in file.readlines() if line.strip()]\n",
    "        \n",
    "        results = {}\n",
    "\n",
    "        for prompt in tqdm(prompts, desc=f\"Processing {filename}\"):\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            with torch.no_grad():\n",
    "                gen_tokens = model.generate(input_ids, **generation_parameters)\n",
    "            generated_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "            results[prompt] = generated_text\n",
    "\n",
    "        json_filename = filename.replace(\"_prompts.txt\", \"_results.json\")\n",
    "        json_file_path = os.path.join(results_folder, json_filename)\n",
    "\n",
    "        with open(json_file_path, \"w\") as json_file:\n",
    "            json.dump(results, json_file, indent=4)\n",
    "\n",
    "        print(f\"Results saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Loan Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_length\": 75,\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "directories = [\"loan_approval_e\", \"loan_approval_g\", \"loan_approval_s\"]\n",
    "\n",
    "for dir_name in directories:\n",
    "    results_folder = os.path.join(dir_name, model_name)\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.endswith(\"_prompts.txt\"):\n",
    "            file_path = os.path.join(dir_name, filename)\n",
    "            \n",
    "            with open(file_path, \"r\") as file:\n",
    "                prompts = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "            results = {}\n",
    "\n",
    "            for prompt in tqdm(prompts, desc=f\"Processing {filename} in {dir_name}\"):\n",
    "                input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    gen_tokens = model.generate(input_ids, **generation_parameters)\n",
    "\n",
    "                generated_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "                results[prompt] = generated_text\n",
    "\n",
    "            json_filename = filename.replace(\"_prompts.txt\", \"_results.json\")\n",
    "            json_file_path = os.path.join(results_folder, json_filename)\n",
    "\n",
    "            with open(json_file_path, \"w\") as json_file:\n",
    "                json.dump(results, json_file, indent=4)\n",
    "\n",
    "            print(f\"Results saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Hiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_length\": 50,\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "hiring_directory = \"hiring\"\n",
    "prompts_file = os.path.join(hiring_directory, \"hiring_prompts.txt\")\n",
    "\n",
    "results_directory = os.path.join(hiring_directory, model_name)\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "with open(prompts_file, \"r\") as file:\n",
    "    prompts = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "results = {}\n",
    "for prompt in tqdm(prompts, desc=\"Processing hiring prompts\"):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model.generate(input_ids, **generation_parameters)\n",
    "\n",
    "    generated_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    results[prompt] = generated_text\n",
    "\n",
    "json_filename = \"hiring_results.json\"\n",
    "json_file_path = os.path.join(results_directory, json_filename)\n",
    "\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(results, json_file, indent=4)\n",
    "\n",
    "print(f\"Results saved to {json_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
